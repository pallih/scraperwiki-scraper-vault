#This is the 3rd and final pass at a scraper of Excel files. 
#Previously it scraped the all sheets of one spreadsheet, identified by URL: https://scraperwiki.com/scrapers/excelscraper_sitreports_multiplesheets/
#A first pass scraped one sheet of one spreadsheet: https://scraperwiki.com/scrapers/excelscraper_sitreports2/

#useful guides at:
#https://scraperwiki.com/docs/python/python_excel_guide/
#http://blog.scraperwiki.com/2011/09/14/scraping-guides-excel-spreadsheets/
#https://scraperwiki.com/docs/python/tutorials/

#TO DO: Separate our header rows from the rest of the sheet
#HOW? A separate for loop that prefixes them?

#TO DO: Check how many cols the biggest sheet has

#TO DO: Create separate record for each spreadsheet/sheet

#TO DO: Simplify the lines record['date3'] = sheet.row_values(rownumber)[6]
#Index should be able to be populated/looped, e.g. [0:20]

#import xlrd library - documentation at https://secure.simplistix.co.uk/svn/xlrd/trunk/xlrd/doc/xlrd.html
import xlrd
import scraperwiki
import lxml.html
import datetime

#create a new function called scrapesheets - it will be called later
#name the parameter passed to it 'XLS'
def scrapesheets(XLS):
    #use the scrape function on XLS to create a new variable 'xlbin'
    xlbin = scraperwiki.scrape(XLS)
    #use the open_workbook function on that new variable to create another, 'book'
    book = xlrd.open_workbook(file_contents=xlbin)
    #the .nsheets method tells us how many sheets 'book' has
    print "nsheets result", book.nsheets
    #create a variable 'id', set at 0. We'll add to this below to create a unique ID for each record in the sheets
    id = 0
    sheet = book.sheet_by_index(0)
#use the row_values method and index (1) to grab the second row of 'sheet', and put all cells into the list variable 'title'
    title = sheet.row_values(1)
#print the string "Title:", followed by the third item (column) in the variable 'title' 
    print "Title:", title[2]
#put cells from the 15th row into 'keys' variable 
    keys = sheet.row_values(14)
    keyslist = []
    for key in keys:
        if isinstance(key, float):
            key = xlrd.xldate_as_tuple(key,0)
            print "date as tuple:",key
        #still cannot convert this using datetime.datetime(key) because generates error saying datetime expects an integer
        #search for 'convert tuple to date python' brings you to solution here: http://www.saltycrane.com/blog/2008/11/python-datetime-time-conversions/
            key = datetime.datetime(*key[0:6])
            print "date as datetime object:", key
            key = key.strftime("%Y.%m.%d")
            print "date converted using .strftime method:", key
            print "date split by full stop, first index (year) grabbed:", key.split(".")[0]
            key = key.split(".")[0]+"_"+key.split(".")[1]+"_"+key.split(".")[2]
            print "concatenated:", key
            #AFTER ALL THIS, STILL NOT 'SIMPLE TEXT', WHICH BREAKS SCRAPER
            print "converted date?", key
            keyslist.append(str(key))
            print keyslist
        else:
            keyslist.append(str(key))

    record = {}
        #loop through a range - from the 18th item (17) to a number generated by using the .nrows method on 'sheet' (to find number of rows in that sheet)
        #put each row number in 'rownumber' as you loop
    Name = "no entry"
    record['title'] = title[2]
    for rownumber in range(17, sheet.nrows):
        print "scraping row ", rownumber
        #the next line generates an error if one of the keys is an unconverted date integer (like 41219.0)
        #we worked this out through eliminating that date key with the following adaptation of an earlier line...
        #for num in range(1,sheet.ncols-5)
        #once eliminated, we know we need to convert that date key earlier - see above
        for num in range(1,sheet.ncols):
            record[keyslist[num]] = sheet.row_values(rownumber)[num]
            id = id+1
        record['id'] = id
        print "---", record
        scraperwiki.sqlite.save([keyslist[2], 'id'], record)

#TO DO: Scrape the base URL and identify all the links to spreadsheets
#Then run the scrape_sheets function above on each one

#define our URL - this links to all the spreadsheets
URL = 'http://transparency.dh.gov.uk/2012/10/26/winter-pressures-daily-situation-reports-2012-13/'

#To test that this scraper runs before creating/running the grabexcellinks function, uncomment the next two lines:
#XLS = 'https://www.wp.dh.gov.uk/transparency/files/2012/10/DailySR-Pub-file-WE-11-11-123.xls'
#scrapesheets(XLS)

#Create a new function which takes one parameter and names it 'URL'
def grabexcellinks(URL):
    #Use Scraperwiki's scrape function on 'URL', put results in new variable 'html'
    html = scraperwiki.scrape(URL)
    #and show it us
    print html
    #Use lxml.html's fromstring function on 'html', put results in new variable 'root'
    root = lxml.html.fromstring(html)
    #use cssselect method on 'root' to grab all <a> tags within a <p> tag - and put in a new list variable 'links'
    links = root.cssselect('p a')
    #for each item in that list variable, from the first to the second last [0:-1], put it in the variable 'link'
    for link in links[0:-1]:
        #and print the text_content of that (after the string "link text:")
        print "link text:", link.text_content()
        #use the attrib.get method on 'link' to grab the href= attribute of the HTML, and put in new 'linkurl' variable
        linkurl = link.attrib.get('href')
        #print it
        print linkurl
        #run the function scrapesheets, using that variable as the parameter
        scrapesheets(linkurl)

grabexcellinks(URL)

#HTML to grab is:
#<p><strong>November 2012</strong><br />
#<a href="https://www.wp.dh.gov.uk/transparency/files/2012/10/DailySR-Web-file-WE-18-11-12.xls">DailySR &#8211; week ending 18 Nov 12.xls (492KB).</a><br />
#<a href="https://www.wp.dh.gov.uk/transparency/files/2012/10/DailySR-Pub-file-WE-11-11-123.xls">DailySR &#8211; 6 Nov 12 to 11 Nov 12.xls (446KB)</a>.</p>

#HTML to avoid that generates an error is:
#<p>Data for this collection is available back to November 2010.<br />
#For previous years’ data <a #href="http://www.dh.gov.uk/en/Publicationsandstatistics/Statistics/Performancedataandstatistics/DailySituationReports/index.htm">click here</a>.</p>
#This is the 3rd and final pass at a scraper of Excel files. 
#Previously it scraped the all sheets of one spreadsheet, identified by URL: https://scraperwiki.com/scrapers/excelscraper_sitreports_multiplesheets/
#A first pass scraped one sheet of one spreadsheet: https://scraperwiki.com/scrapers/excelscraper_sitreports2/

#useful guides at:
#https://scraperwiki.com/docs/python/python_excel_guide/
#http://blog.scraperwiki.com/2011/09/14/scraping-guides-excel-spreadsheets/
#https://scraperwiki.com/docs/python/tutorials/

#TO DO: Separate our header rows from the rest of the sheet
#HOW? A separate for loop that prefixes them?

#TO DO: Check how many cols the biggest sheet has

#TO DO: Create separate record for each spreadsheet/sheet

#TO DO: Simplify the lines record['date3'] = sheet.row_values(rownumber)[6]
#Index should be able to be populated/looped, e.g. [0:20]

#import xlrd library - documentation at https://secure.simplistix.co.uk/svn/xlrd/trunk/xlrd/doc/xlrd.html
import xlrd
import scraperwiki
import lxml.html
import datetime

#create a new function called scrapesheets - it will be called later
#name the parameter passed to it 'XLS'
def scrapesheets(XLS):
    #use the scrape function on XLS to create a new variable 'xlbin'
    xlbin = scraperwiki.scrape(XLS)
    #use the open_workbook function on that new variable to create another, 'book'
    book = xlrd.open_workbook(file_contents=xlbin)
    #the .nsheets method tells us how many sheets 'book' has
    print "nsheets result", book.nsheets
    #create a variable 'id', set at 0. We'll add to this below to create a unique ID for each record in the sheets
    id = 0
    sheet = book.sheet_by_index(0)
#use the row_values method and index (1) to grab the second row of 'sheet', and put all cells into the list variable 'title'
    title = sheet.row_values(1)
#print the string "Title:", followed by the third item (column) in the variable 'title' 
    print "Title:", title[2]
#put cells from the 15th row into 'keys' variable 
    keys = sheet.row_values(14)
    keyslist = []
    for key in keys:
        if isinstance(key, float):
            key = xlrd.xldate_as_tuple(key,0)
            print "date as tuple:",key
        #still cannot convert this using datetime.datetime(key) because generates error saying datetime expects an integer
        #search for 'convert tuple to date python' brings you to solution here: http://www.saltycrane.com/blog/2008/11/python-datetime-time-conversions/
            key = datetime.datetime(*key[0:6])
            print "date as datetime object:", key
            key = key.strftime("%Y.%m.%d")
            print "date converted using .strftime method:", key
            print "date split by full stop, first index (year) grabbed:", key.split(".")[0]
            key = key.split(".")[0]+"_"+key.split(".")[1]+"_"+key.split(".")[2]
            print "concatenated:", key
            #AFTER ALL THIS, STILL NOT 'SIMPLE TEXT', WHICH BREAKS SCRAPER
            print "converted date?", key
            keyslist.append(str(key))
            print keyslist
        else:
            keyslist.append(str(key))

    record = {}
        #loop through a range - from the 18th item (17) to a number generated by using the .nrows method on 'sheet' (to find number of rows in that sheet)
        #put each row number in 'rownumber' as you loop
    Name = "no entry"
    record['title'] = title[2]
    for rownumber in range(17, sheet.nrows):
        print "scraping row ", rownumber
        #the next line generates an error if one of the keys is an unconverted date integer (like 41219.0)
        #we worked this out through eliminating that date key with the following adaptation of an earlier line...
        #for num in range(1,sheet.ncols-5)
        #once eliminated, we know we need to convert that date key earlier - see above
        for num in range(1,sheet.ncols):
            record[keyslist[num]] = sheet.row_values(rownumber)[num]
            id = id+1
        record['id'] = id
        print "---", record
        scraperwiki.sqlite.save([keyslist[2], 'id'], record)

#TO DO: Scrape the base URL and identify all the links to spreadsheets
#Then run the scrape_sheets function above on each one

#define our URL - this links to all the spreadsheets
URL = 'http://transparency.dh.gov.uk/2012/10/26/winter-pressures-daily-situation-reports-2012-13/'

#To test that this scraper runs before creating/running the grabexcellinks function, uncomment the next two lines:
#XLS = 'https://www.wp.dh.gov.uk/transparency/files/2012/10/DailySR-Pub-file-WE-11-11-123.xls'
#scrapesheets(XLS)

#Create a new function which takes one parameter and names it 'URL'
def grabexcellinks(URL):
    #Use Scraperwiki's scrape function on 'URL', put results in new variable 'html'
    html = scraperwiki.scrape(URL)
    #and show it us
    print html
    #Use lxml.html's fromstring function on 'html', put results in new variable 'root'
    root = lxml.html.fromstring(html)
    #use cssselect method on 'root' to grab all <a> tags within a <p> tag - and put in a new list variable 'links'
    links = root.cssselect('p a')
    #for each item in that list variable, from the first to the second last [0:-1], put it in the variable 'link'
    for link in links[0:-1]:
        #and print the text_content of that (after the string "link text:")
        print "link text:", link.text_content()
        #use the attrib.get method on 'link' to grab the href= attribute of the HTML, and put in new 'linkurl' variable
        linkurl = link.attrib.get('href')
        #print it
        print linkurl
        #run the function scrapesheets, using that variable as the parameter
        scrapesheets(linkurl)

grabexcellinks(URL)

#HTML to grab is:
#<p><strong>November 2012</strong><br />
#<a href="https://www.wp.dh.gov.uk/transparency/files/2012/10/DailySR-Web-file-WE-18-11-12.xls">DailySR &#8211; week ending 18 Nov 12.xls (492KB).</a><br />
#<a href="https://www.wp.dh.gov.uk/transparency/files/2012/10/DailySR-Pub-file-WE-11-11-123.xls">DailySR &#8211; 6 Nov 12 to 11 Nov 12.xls (446KB)</a>.</p>

#HTML to avoid that generates an error is:
#<p>Data for this collection is available back to November 2010.<br />
#For previous years’ data <a #href="http://www.dh.gov.uk/en/Publicationsandstatistics/Statistics/Performancedataandstatistics/DailySituationReports/index.htm">click here</a>.</p>
