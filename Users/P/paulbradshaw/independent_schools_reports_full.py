#ERROR generated by http://www.isi.net/schools/8843/
#if pdflinks[0].attrib.get('href') is not None:
#IndexError: list index out of range

#Solves the problem of data being hidden behind a search form
#Does this by using the mechanize library to mimic a browser and store the results of search
#Before scraping the table of search results 

import scraperwiki
import mechanize
from BeautifulSoup import BeautifulSoup
import lxml.html
import urllib2
import re

def scrapepdf(pdflink,schoolpage,table_cells,rows):
    print "scraping PDF:", pdflink
    record = {}
    if pdflink[-3:] == "htm":
        print "HTML PAGE"
        charityno = "NOT SCRAPED - NO PDF"
    elif pdflink is None:
        charityno = "NOT SCRAPED - NO PDF"
    else:
    #Start running a PDF scraper on that, firstly 'opening' the PDF URL with the urllib2 library's urlopen function, and 'reading' the PDF into a variable called 'pdfdata'...
        pdfdata = urllib2.urlopen(pdflink).read()
#...Then using pdftoxml to convert that into a variable called 'xmldata'
        xmldata = scraperwiki.pdftoxml(pdfdata)
    #...Then using .fromstring to convert that into a variable called 'pdfxml'
        pdfxml = lxml.etree.fromstring(xmldata)
        print xmldata
        boldtags = pdfxml.xpath('.//text')
        linenumber = 0
        #create a new record to contain the data
        for heading in boldtags:
            linenumber = linenumber+1
            #print "Heading:", heading.text
            if heading.text is not None:
                mention = re.match(r'.*Charity Number.*',heading.text)
                if mention:
                    print "FULL LINE", lxml.etree.tostring(heading, encoding="unicode", method="text")
                    charityno = lxml.etree.tostring(heading, encoding="unicode", method="text")
                else:
                    charityno = "NO CHARITY NUMBER"
                record['charityno'] = charityno
    #all this data was passed to this function from the results page using the table_cells parameter
    record['Name'] = table_cells[0].text
    record['Town'] = table_cells[1].text
    record['Country'] = table_cells[2].text
    record['Year'] = table_cells[3].text
    record['Ages'] = table_cells[4].text
    record['Gender'] = table_cells[5].text
    record['URL'] = schoolpage
    # Print out the data we've gathered
    print record, '------------'
    # the rest is from this page
    record['Postcode2'] = rows[3].text
    record['Town2'] = rows[1].text
    record['Ages2'] = rows[5].text
    record['Day_board2'] = rows[7].text
    record['Gender2'] = rows[9].text
    record['ID'] = schoolpage
    print record, '------------'
    scraperwiki.sqlite.save(["ID"], record)               

def scrapeschoolpage(schoolpage,table_cells):
    print "scraping:", schoolpage
    html = scraperwiki.scrape(schoolpage)
    root = lxml.html.fromstring(html)
    #Use cssselect to find the contents of a particular HTML tag, and put it in a new object 'rows'
    #there's more than one div, so we need to specify the class="tabular", 'class' represented by the full stop
    #there's also more than one <tr> within that div, and more than one <td>... 
    #'rows' will contain all 10 <td>s as a list, and we'll grab the ones we want in turn
    rows = root.cssselect("div.tabular tr td")
    print rows
    pdflinks = root.cssselect("a.'blue new'")
    if pdflinks[0].attrib.get('href') is not None:
        pdflink = pdflinks[0].attrib.get('href')
        scrapepdf(pdflink,schoolpage,table_cells,rows)
    else:
        pdflink = "NONE"

    
#UP TO HERE

#create scrape_table function: gets passed an individual page (soup) to scrape 
def scrape_table(soup):
    #find table class="reports"
    print soup
    data_table = soup.find("table", { "class" : "reports" })
    #find each table row <tr>
    rows = data_table.findAll("tr")
    #for each row, loop through this
    for row in rows:
        #create a record to hold the data
        record = {}
        #find each cell <td>
        table_cells = row.findAll("td")
        #if there is a cell, record the contents in our dataset, the first cell [0] in 'Name' and so on
        if table_cells: 
            schoolpage = table_cells[0].findAll("a")[0]['href']
            scrapeschoolpage(schoolpage,table_cells)

#START HERE

#set the URL containing the form we need to open with mechanize
starting_url = 'http://www.isi.net/reports/'
br = mechanize.Browser()
# Set the user-agent as Mozilla - if the page knows we're Mechanize, it won't return all fields
br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]
#open the URL previously defined as 'starting_url'
br.open(starting_url)
#find out and display (print) the names of any forms in the HTML
#i.e. <form ... name="
print "All forms:", [ form.name  for form in br.forms() ]
#as it happens, the name of the form in this page is... "form"
br.select_form(name="form")
#submit the form and put the contents into 'response'
response = br.submit()
#create soup object by reading the contents of response and passing it through BeautifulSoup
soup = BeautifulSoup(br.response().read())
#If we were using LXML then here's the alternative code
#root = lxml.html.fromstring(response.read())
#see http://www.winningmark.com/2012/03/02/scraperwiki-helping-make-data-more-open/
# Have a look at 'soup': note the 'onSubmit' JavaScript function that is called when 
# you click on the 'next' link. We'll mimic this in the function above.
print soup 
# start scraping by running function created above, which in turn runs the other function
#scrape_and_look_for_next_link(soup)
scrape_table(soup)


#if we need to print contents of form so we can see what it contains before next step
#print br.form
#if the form requires certain fields to be filled/selected, then we would do so here
#like so: br["ctl00$phMainContent$dropDownAwardDate"] = ["Between"]
#see https://scraperwiki.com/scrapers/new/python?template=tutorial-mechanize#
#but we can skip this step and submit this one empty

#assign the 'unpacked' (read) contents of 'response' to new object: 'allschools'
#allschools = response.read()
#print the contents of 'allschools' 
#print allschools

#scraping http://www.isi.net/reports/
#Empty search brings up list of schools with links to pages, e.g. http://www.isi.net/schools/6173/
#Each page contains ASPX link to PDF report, e.g. http://reports.isi.net/DownloadReport.aspx?s=6173&t=c
#Source code: <a href="http://reports.isi.net/DownloadReport.aspx?s=6173&t=c" class="blue" target="_blank">View Inspection Report<!-- <span class="grey">&rsaquo;</span>--></a>
#Also link to school info page http://www.isc.co.uk/school_73499.htm
#<p><a href="http://www.isc.co.uk/school_73499.htm" target="_blank" class="view-button">View school information <span class="blue"><strong>&rsaquo;</strong></span></a></p>
#PDF has no numerical data but may be able to extract particular language, e.g. "excellent", "satisfactory"
#One section may be worth scraping specifically:
#Compliance with statutory requirements for children under three
#6.5 The school’s registered provision for childcare meets the requirements of the
#Childcare Act 2006 and no action is required.
#Complaints since the last inspection
#6.6 Since the last inspection, there have been no complaints made to Ofsted that
#required any action to meet national requirements
#ERROR generated by http://www.isi.net/schools/8843/
#if pdflinks[0].attrib.get('href') is not None:
#IndexError: list index out of range

#Solves the problem of data being hidden behind a search form
#Does this by using the mechanize library to mimic a browser and store the results of search
#Before scraping the table of search results 

import scraperwiki
import mechanize
from BeautifulSoup import BeautifulSoup
import lxml.html
import urllib2
import re

def scrapepdf(pdflink,schoolpage,table_cells,rows):
    print "scraping PDF:", pdflink
    record = {}
    if pdflink[-3:] == "htm":
        print "HTML PAGE"
        charityno = "NOT SCRAPED - NO PDF"
    elif pdflink is None:
        charityno = "NOT SCRAPED - NO PDF"
    else:
    #Start running a PDF scraper on that, firstly 'opening' the PDF URL with the urllib2 library's urlopen function, and 'reading' the PDF into a variable called 'pdfdata'...
        pdfdata = urllib2.urlopen(pdflink).read()
#...Then using pdftoxml to convert that into a variable called 'xmldata'
        xmldata = scraperwiki.pdftoxml(pdfdata)
    #...Then using .fromstring to convert that into a variable called 'pdfxml'
        pdfxml = lxml.etree.fromstring(xmldata)
        print xmldata
        boldtags = pdfxml.xpath('.//text')
        linenumber = 0
        #create a new record to contain the data
        for heading in boldtags:
            linenumber = linenumber+1
            #print "Heading:", heading.text
            if heading.text is not None:
                mention = re.match(r'.*Charity Number.*',heading.text)
                if mention:
                    print "FULL LINE", lxml.etree.tostring(heading, encoding="unicode", method="text")
                    charityno = lxml.etree.tostring(heading, encoding="unicode", method="text")
                else:
                    charityno = "NO CHARITY NUMBER"
                record['charityno'] = charityno
    #all this data was passed to this function from the results page using the table_cells parameter
    record['Name'] = table_cells[0].text
    record['Town'] = table_cells[1].text
    record['Country'] = table_cells[2].text
    record['Year'] = table_cells[3].text
    record['Ages'] = table_cells[4].text
    record['Gender'] = table_cells[5].text
    record['URL'] = schoolpage
    # Print out the data we've gathered
    print record, '------------'
    # the rest is from this page
    record['Postcode2'] = rows[3].text
    record['Town2'] = rows[1].text
    record['Ages2'] = rows[5].text
    record['Day_board2'] = rows[7].text
    record['Gender2'] = rows[9].text
    record['ID'] = schoolpage
    print record, '------------'
    scraperwiki.sqlite.save(["ID"], record)               

def scrapeschoolpage(schoolpage,table_cells):
    print "scraping:", schoolpage
    html = scraperwiki.scrape(schoolpage)
    root = lxml.html.fromstring(html)
    #Use cssselect to find the contents of a particular HTML tag, and put it in a new object 'rows'
    #there's more than one div, so we need to specify the class="tabular", 'class' represented by the full stop
    #there's also more than one <tr> within that div, and more than one <td>... 
    #'rows' will contain all 10 <td>s as a list, and we'll grab the ones we want in turn
    rows = root.cssselect("div.tabular tr td")
    print rows
    pdflinks = root.cssselect("a.'blue new'")
    if pdflinks[0].attrib.get('href') is not None:
        pdflink = pdflinks[0].attrib.get('href')
        scrapepdf(pdflink,schoolpage,table_cells,rows)
    else:
        pdflink = "NONE"

    
#UP TO HERE

#create scrape_table function: gets passed an individual page (soup) to scrape 
def scrape_table(soup):
    #find table class="reports"
    print soup
    data_table = soup.find("table", { "class" : "reports" })
    #find each table row <tr>
    rows = data_table.findAll("tr")
    #for each row, loop through this
    for row in rows:
        #create a record to hold the data
        record = {}
        #find each cell <td>
        table_cells = row.findAll("td")
        #if there is a cell, record the contents in our dataset, the first cell [0] in 'Name' and so on
        if table_cells: 
            schoolpage = table_cells[0].findAll("a")[0]['href']
            scrapeschoolpage(schoolpage,table_cells)

#START HERE

#set the URL containing the form we need to open with mechanize
starting_url = 'http://www.isi.net/reports/'
br = mechanize.Browser()
# Set the user-agent as Mozilla - if the page knows we're Mechanize, it won't return all fields
br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]
#open the URL previously defined as 'starting_url'
br.open(starting_url)
#find out and display (print) the names of any forms in the HTML
#i.e. <form ... name="
print "All forms:", [ form.name  for form in br.forms() ]
#as it happens, the name of the form in this page is... "form"
br.select_form(name="form")
#submit the form and put the contents into 'response'
response = br.submit()
#create soup object by reading the contents of response and passing it through BeautifulSoup
soup = BeautifulSoup(br.response().read())
#If we were using LXML then here's the alternative code
#root = lxml.html.fromstring(response.read())
#see http://www.winningmark.com/2012/03/02/scraperwiki-helping-make-data-more-open/
# Have a look at 'soup': note the 'onSubmit' JavaScript function that is called when 
# you click on the 'next' link. We'll mimic this in the function above.
print soup 
# start scraping by running function created above, which in turn runs the other function
#scrape_and_look_for_next_link(soup)
scrape_table(soup)


#if we need to print contents of form so we can see what it contains before next step
#print br.form
#if the form requires certain fields to be filled/selected, then we would do so here
#like so: br["ctl00$phMainContent$dropDownAwardDate"] = ["Between"]
#see https://scraperwiki.com/scrapers/new/python?template=tutorial-mechanize#
#but we can skip this step and submit this one empty

#assign the 'unpacked' (read) contents of 'response' to new object: 'allschools'
#allschools = response.read()
#print the contents of 'allschools' 
#print allschools

#scraping http://www.isi.net/reports/
#Empty search brings up list of schools with links to pages, e.g. http://www.isi.net/schools/6173/
#Each page contains ASPX link to PDF report, e.g. http://reports.isi.net/DownloadReport.aspx?s=6173&t=c
#Source code: <a href="http://reports.isi.net/DownloadReport.aspx?s=6173&t=c" class="blue" target="_blank">View Inspection Report<!-- <span class="grey">&rsaquo;</span>--></a>
#Also link to school info page http://www.isc.co.uk/school_73499.htm
#<p><a href="http://www.isc.co.uk/school_73499.htm" target="_blank" class="view-button">View school information <span class="blue"><strong>&rsaquo;</strong></span></a></p>
#PDF has no numerical data but may be able to extract particular language, e.g. "excellent", "satisfactory"
#One section may be worth scraping specifically:
#Compliance with statutory requirements for children under three
#6.5 The school’s registered provision for childcare meets the requirements of the
#Childcare Act 2006 and no action is required.
#Complaints since the last inspection
#6.6 Since the last inspection, there have been no complaints made to Ofsted that
#required any action to meet national requirements
#ERROR generated by http://www.isi.net/schools/8843/
#if pdflinks[0].attrib.get('href') is not None:
#IndexError: list index out of range

#Solves the problem of data being hidden behind a search form
#Does this by using the mechanize library to mimic a browser and store the results of search
#Before scraping the table of search results 

import scraperwiki
import mechanize
from BeautifulSoup import BeautifulSoup
import lxml.html
import urllib2
import re

def scrapepdf(pdflink,schoolpage,table_cells,rows):
    print "scraping PDF:", pdflink
    record = {}
    if pdflink[-3:] == "htm":
        print "HTML PAGE"
        charityno = "NOT SCRAPED - NO PDF"
    elif pdflink is None:
        charityno = "NOT SCRAPED - NO PDF"
    else:
    #Start running a PDF scraper on that, firstly 'opening' the PDF URL with the urllib2 library's urlopen function, and 'reading' the PDF into a variable called 'pdfdata'...
        pdfdata = urllib2.urlopen(pdflink).read()
#...Then using pdftoxml to convert that into a variable called 'xmldata'
        xmldata = scraperwiki.pdftoxml(pdfdata)
    #...Then using .fromstring to convert that into a variable called 'pdfxml'
        pdfxml = lxml.etree.fromstring(xmldata)
        print xmldata
        boldtags = pdfxml.xpath('.//text')
        linenumber = 0
        #create a new record to contain the data
        for heading in boldtags:
            linenumber = linenumber+1
            #print "Heading:", heading.text
            if heading.text is not None:
                mention = re.match(r'.*Charity Number.*',heading.text)
                if mention:
                    print "FULL LINE", lxml.etree.tostring(heading, encoding="unicode", method="text")
                    charityno = lxml.etree.tostring(heading, encoding="unicode", method="text")
                else:
                    charityno = "NO CHARITY NUMBER"
                record['charityno'] = charityno
    #all this data was passed to this function from the results page using the table_cells parameter
    record['Name'] = table_cells[0].text
    record['Town'] = table_cells[1].text
    record['Country'] = table_cells[2].text
    record['Year'] = table_cells[3].text
    record['Ages'] = table_cells[4].text
    record['Gender'] = table_cells[5].text
    record['URL'] = schoolpage
    # Print out the data we've gathered
    print record, '------------'
    # the rest is from this page
    record['Postcode2'] = rows[3].text
    record['Town2'] = rows[1].text
    record['Ages2'] = rows[5].text
    record['Day_board2'] = rows[7].text
    record['Gender2'] = rows[9].text
    record['ID'] = schoolpage
    print record, '------------'
    scraperwiki.sqlite.save(["ID"], record)               

def scrapeschoolpage(schoolpage,table_cells):
    print "scraping:", schoolpage
    html = scraperwiki.scrape(schoolpage)
    root = lxml.html.fromstring(html)
    #Use cssselect to find the contents of a particular HTML tag, and put it in a new object 'rows'
    #there's more than one div, so we need to specify the class="tabular", 'class' represented by the full stop
    #there's also more than one <tr> within that div, and more than one <td>... 
    #'rows' will contain all 10 <td>s as a list, and we'll grab the ones we want in turn
    rows = root.cssselect("div.tabular tr td")
    print rows
    pdflinks = root.cssselect("a.'blue new'")
    if pdflinks[0].attrib.get('href') is not None:
        pdflink = pdflinks[0].attrib.get('href')
        scrapepdf(pdflink,schoolpage,table_cells,rows)
    else:
        pdflink = "NONE"

    
#UP TO HERE

#create scrape_table function: gets passed an individual page (soup) to scrape 
def scrape_table(soup):
    #find table class="reports"
    print soup
    data_table = soup.find("table", { "class" : "reports" })
    #find each table row <tr>
    rows = data_table.findAll("tr")
    #for each row, loop through this
    for row in rows:
        #create a record to hold the data
        record = {}
        #find each cell <td>
        table_cells = row.findAll("td")
        #if there is a cell, record the contents in our dataset, the first cell [0] in 'Name' and so on
        if table_cells: 
            schoolpage = table_cells[0].findAll("a")[0]['href']
            scrapeschoolpage(schoolpage,table_cells)

#START HERE

#set the URL containing the form we need to open with mechanize
starting_url = 'http://www.isi.net/reports/'
br = mechanize.Browser()
# Set the user-agent as Mozilla - if the page knows we're Mechanize, it won't return all fields
br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]
#open the URL previously defined as 'starting_url'
br.open(starting_url)
#find out and display (print) the names of any forms in the HTML
#i.e. <form ... name="
print "All forms:", [ form.name  for form in br.forms() ]
#as it happens, the name of the form in this page is... "form"
br.select_form(name="form")
#submit the form and put the contents into 'response'
response = br.submit()
#create soup object by reading the contents of response and passing it through BeautifulSoup
soup = BeautifulSoup(br.response().read())
#If we were using LXML then here's the alternative code
#root = lxml.html.fromstring(response.read())
#see http://www.winningmark.com/2012/03/02/scraperwiki-helping-make-data-more-open/
# Have a look at 'soup': note the 'onSubmit' JavaScript function that is called when 
# you click on the 'next' link. We'll mimic this in the function above.
print soup 
# start scraping by running function created above, which in turn runs the other function
#scrape_and_look_for_next_link(soup)
scrape_table(soup)


#if we need to print contents of form so we can see what it contains before next step
#print br.form
#if the form requires certain fields to be filled/selected, then we would do so here
#like so: br["ctl00$phMainContent$dropDownAwardDate"] = ["Between"]
#see https://scraperwiki.com/scrapers/new/python?template=tutorial-mechanize#
#but we can skip this step and submit this one empty

#assign the 'unpacked' (read) contents of 'response' to new object: 'allschools'
#allschools = response.read()
#print the contents of 'allschools' 
#print allschools

#scraping http://www.isi.net/reports/
#Empty search brings up list of schools with links to pages, e.g. http://www.isi.net/schools/6173/
#Each page contains ASPX link to PDF report, e.g. http://reports.isi.net/DownloadReport.aspx?s=6173&t=c
#Source code: <a href="http://reports.isi.net/DownloadReport.aspx?s=6173&t=c" class="blue" target="_blank">View Inspection Report<!-- <span class="grey">&rsaquo;</span>--></a>
#Also link to school info page http://www.isc.co.uk/school_73499.htm
#<p><a href="http://www.isc.co.uk/school_73499.htm" target="_blank" class="view-button">View school information <span class="blue"><strong>&rsaquo;</strong></span></a></p>
#PDF has no numerical data but may be able to extract particular language, e.g. "excellent", "satisfactory"
#One section may be worth scraping specifically:
#Compliance with statutory requirements for children under three
#6.5 The school’s registered provision for childcare meets the requirements of the
#Childcare Act 2006 and no action is required.
#Complaints since the last inspection
#6.6 Since the last inspection, there have been no complaints made to Ofsted that
#required any action to meet national requirements
